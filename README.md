# Sample-GPT

Sample-GPT is a proof-of-concept project that demonstrates building a GPT-like model using PyTorch. This repository is designed for educational purposes and experimentation, providing a simple yet effective implementation of a transformer-based language model.

## Repository Structure

The repository is organized into the following folders:

* **micrograd** : Contains code related to micrograd implementation.
* **v1** : Includes the first version of the GPT model implementation.
* **v2** : Contains the second version of the GPT model with improvements and additional features.

Each folder has its own README file that provides detailed information about its contents, usage instructions, and relevant notes.

## Getting Started

To get started with Sample-GPT, follow these steps:

1. **Clone the repository** :

```bash
   git clone https://github.com/dash7ou/Sample-GPT.git
   cd Sample-GPT
```

1. **Install dependencies** :
   Ensure you have Python installed, then install the required packages:

```bash
   pip install -r requirements.txt
```

1. **Explore the folders** :
   Navigate to each folder and refer to their respective README files for specific instructions and information.

## Contributing

Contributions are welcome! If you have suggestions or improvements, feel free to open an issue or submit a pull request.

## License

This project is licensed under the MIT License.

---

*Note: Each folder within this repository contains its own README file with detailed information specific to that section.*
